{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1434765a",
   "metadata": {},
   "source": [
    "# LLMs and CSS\n",
    "\n",
    "In this short notebook, we'll see how LLMs can be used to annotate and generate data for studying linguistic constructs in text.\n",
    "\n",
    "Let's break down the goals:\n",
    "\n",
    "* **Goal 1:** Learn langchain.\n",
    "    * Langchain is the most common library for interacting with LLMs in Python!\n",
    "\n",
    "* **Goal 2:** Annotate a dataset with ChatGPT\n",
    "    * We'll annotate texts as being sarcastic or not using ChatGPT.\n",
    "* **Goal 3:** Generate Synthetic Data for a Specific Construct on Interest\n",
    "    * We'll generate new examples of sarcastic and non-sarcastic texts.\n",
    "    \n",
    "A few requirements.\n",
    "1. You will need an OpenAI key to generate the data. Since the data has already been generated, you won't need it to explore the synthetic data, but if you want to re-run the generation you will need to get a key. You can signup [here](https://openai.com/blog/openai-api)\n",
    "2. *(If you have an API key)* In the .env file in root add your API key.\n",
    "3. Run the requirements.txt file to pip install all the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe681c6",
   "metadata": {},
   "source": [
    "### Local Setup\n",
    "Let's install all the required libraries to go through this document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"requirements.txt\"\n",
    "!pip install -r {requirements}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14b11be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from utils import *\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95040cc2",
   "metadata": {},
   "source": [
    "Run this cell to load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db64837",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e793b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42    # for reproducibility \n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fda0e",
   "metadata": {},
   "source": [
    "## 1. Dataset Introduction\n",
    "\n",
    "The dataset includes two columns: `text` and `labels`. Where `text` is a Tweet and `label` is either sarcastic or non-sarcastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c9a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/sarcasm.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0838c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  500\n",
      "Number of sarcastic comments:  135\n",
      "\n",
      "Example of a sarcastic text\n",
      "do people with clear skin feel accomplished?? superior??? comfortable in their own skin???? whats that like lmfao\n",
      "\n",
      "Example of a non-sarcastic text\n",
      "A message to all Muslims and Refugees: I'm sorry for how my country is treating you. You are only human. #RefugeesDetained #Trump #rt\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: \",len(df))\n",
    "print(\"Number of sarcastic comments: \",len(df[df[\"labels\"]==\"sarcastic\"]))\n",
    "print()\n",
    "example_rows(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e3713",
   "metadata": {},
   "source": [
    "## 2. Langchain\n",
    "> [LangChain](https://python.langchain.com/docs/get_started/quickstart) is a framework for developing applications powered by language models. \n",
    "\n",
    "We will use Langchain to *annotate* and *generate* sarcastic texts! Langchain is currently the most widely used Python library for interacting with these LLMs programatically. It opens up a lot of cool functionalities, but we will limit to a simple case: given a prompt, let's generate text!\n",
    "\n",
    "To design prompts we need to add both a `system` prompt and a `message` prompt. In Langchain this corresponds `HumanMessagePromptTemplate` and `SytemMessagePromptTemplate`. To read more about prompt templates you can look at the Langchain documentation [here](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/).\n",
    "\n",
    "All the prompts are included in the `utils.py` file but we will add an example below.\n",
    "\n",
    "The **system** message basically puts the model into a certain headspace through meta-instructions. E.g., \"You are a helpful assistant!\".\n",
    "\n",
    "The **human message** instead includes the actual task explanation. \n",
    "\n",
    "In this code, we ask the model to generate `{num_generations}` (for example 10) `{direction}` (for example sarcastic) comments. \n",
    "\n",
    "The function will then return a list with the two messages which we will feed into Langchain's LLM. :)\n",
    "\n",
    "\n",
    "```py\n",
    "def sarcasm_simple_prompt(self) -> list:\n",
    "    system_message = SystemMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[],\n",
    "            template=\"You are a model that generates sarcastic and non-sarcastic texts.\"\n",
    "        )\n",
    "    )\n",
    "    human_message = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[\"num_generations\", \"direction\"],\n",
    "            template=\"Generate {num_generations} {direction} texts. Ensure diversity in the generated texts.\"\n",
    "        )\n",
    "    )\n",
    "    return [system_message, human_message]\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25defd07",
   "metadata": {},
   "source": [
    "### 2.1 Annotations\n",
    "Let's annotate texts as being sarcastic or not, and reporting the performance!\n",
    "\n",
    "First, let's try it on one text:\n",
    "> do people with clear skin feel accomplished?? superior??? comfortable in their own skin???? whats that like lmfao\n",
    "\n",
    "*Only run the code if you have an OpenAI key, otherwise just import the files with already generated data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6a29a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"do people with clear skin feel accomplished?? superior??? comfortable in their own skin???? whats that like lmfao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90f3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9)\n",
    "prompt = ChatPromptTemplate.from_messages(sarcasm_annotate_prompt())\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "generated = chain.run({\"text\": example_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0036c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic\n"
     ]
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d5174",
   "metadata": {},
   "source": [
    "Great! It seems to work. Now we will iterate through all the sarcastic texts in our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea156e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = []\n",
    "for i, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    prompt = ChatPromptTemplate.from_messages(sarcasm_annotate_prompt())\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    generated.append(chain.run({\"text\": example_text}))\n",
    "    \n",
    "df[\"predict\"] = generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e291789",
   "metadata": {},
   "source": [
    "The annotations have already been run, so let's just import the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23eeb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/annotate_gpt-3.5-turbo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1d5e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(x):\n",
    "    \"\"\"\n",
    "    Process GPT outputs. Otherwise \n",
    "    \"\"\"\n",
    "    if \"non-sarcastic\" in x.lower():\n",
    "        return \"not-sarcastic\"\n",
    "    else:\n",
    "        return \"sarcastic\"\n",
    "    \n",
    "df[\"predict\"] = df[\"predict\"].apply(lambda x: process_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165842fa",
   "metadata": {},
   "source": [
    "Let's import some metrics to see how well the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc1f392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.602\n",
      "F1 score: 0.596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "accuracy = accuracy_score(df[\"target\"], df[\"predict\"])\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(df[\"target\"], df[\"predict\"], average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 score: {round(f1, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b7078",
   "metadata": {},
   "source": [
    "Not amazing!\n",
    "\n",
    "Instead, we can try to generate more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076544e",
   "metadata": {},
   "source": [
    "### 2.2 Generating data\n",
    "Now we'll quickly go over how to generate more sarcastic texts. This can be used for *de-novo* dataset creation or for data augmentation. \n",
    "\n",
    "We'll use a grounded prompting technique, where we'll rewrite real tweets to make them sarcastic or not!\n",
    "\n",
    "Let's rewrite this Tweet as an example:\n",
    "> Tapping a tuning fork and seeing who resonates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5b9189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Tapping a tuning fork and seeing who resonates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0112434",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9, max_tokens=512)\n",
    "prompt = ChatPromptTemplate.from_messages(sarcasm_grounded_prompt())\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "generated = chain.run({\"text\": example_text, \"direction\": \"sarcastic\", \"num_generations\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdda10a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh yeah, because tapping a tuning fork and seeing who resonates is clearly the pinnacle of intellectual pursuits. '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37517c79",
   "metadata": {},
   "source": [
    "Don't run the following code if OpenAI key not connected, just import csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68502c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = []\n",
    "for i, row in df.iterrows():\n",
    "    for direction in [\"sarcastic\", \"not-sarcastic\"]:\n",
    "        text = row[\"text\"]\n",
    "        prompt = ChatPromptTemplate.from_messages(sarcasm_grounded_prompt())\n",
    "        chain = LLMChain(prompt=prompt, llm=llm)\n",
    "        generated.append(chain.run({\"text\": example_text, \"direction\": direction}))\n",
    "    \n",
    "df[\"augmented_text\"] = generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fefb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/grounded_gpt-3.5-turbo.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb96a70",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's see, if there are any ideosyncracies in the generated sarcastic texts!\n",
    "\n",
    "There is a lot that can be done here, but we will look at the prevelance of \"Oh\" in sarcastic comments between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebfefb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sarcastic = df[df[\"labels\"]==\"sarcastic\"][\"augmented_text\"].values\n",
    "original_sarcastic = df[df[\"target\"]==\"sarcastic\"].drop_duplicates(subset=\"text\")[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40bc4573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Oh' present in 0.165 of synthetic texts\n",
      "\n",
      "'Oh' present in 0.022 of real texts\n"
     ]
    }
   ],
   "source": [
    "oh_synthetic = len([k for k in generated_sarcastic if \"oh\" in k.lower()]) / len(generated_sarcastic)\n",
    "oh_real = len([k for k in original_sarcastic if \"oh\" in k.lower()]) / len(original_sarcastic)\n",
    "\n",
    "print(f\"'Oh' present in {round(oh_synthetic, 3)} of synthetic texts\")\n",
    "print()\n",
    "print(f\"'Oh' present in {round(oh_real, 3)} of real texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065fe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
